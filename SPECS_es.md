# Especificaciones del Proyecto docs-to-rag

## ğŸ“‹ Resumen del Proyecto

**docs-to-rag** es un proyecto educativo diseÃ±ado para aprender los fundamentos de extracciÃ³n de documentos y creaciÃ³n de bases de conocimiento usando bases de datos vectoriales. El proyecto implementa un pipeline completo de RAG (GeneraciÃ³n Aumentada por RecuperaciÃ³n) que funciona completamente en local usando Ollama tanto para embeddings como para generaciÃ³n de chat.

MÃ¡s allÃ¡ de la funcionalidad bÃ¡sica de RAG, este proyecto incluye **enriquecimiento jerÃ¡rquico de documentos** - una tÃ©cnica avanzada que genera resÃºmenes sintÃ©ticos de documentos en mÃºltiples niveles de abstracciÃ³n (documento, capÃ­tulo y concepto) para mejorar dramÃ¡ticamente la calidad de recuperaciÃ³n para preguntas conceptuales y amplias.

### ğŸ¯ Objetivos Principales

- Aprender extracciÃ³n de documentos de mÃºltiples formatos (PDF, Markdown, HTML)
- Entender embeddings vectoriales y bÃºsqueda por similitud
- Implementar un pipeline completo de RAG
- Practicar integraciÃ³n con LLM locales usando Ollama
- Construir una herramienta CLI funcional para procesamiento y consulta de documentos
- Explorar tÃ©cnicas avanzadas de RAG a travÃ©s de resumen jerÃ¡rquico de documentos

### ğŸ”§ Requisitos Clave

- **100% Local**: No requiere APIs externas
- **Enfoque Educativo**: CÃ³digo claro y bien documentado con objetivos de aprendizaje
- **Simple pero Funcional**: ImplementaciÃ³n minimalista que funciona efectivamente
- **Interfaz CLI**: Herramienta de lÃ­nea de comandos para interacciÃ³n fÃ¡cil
- **MÃºltiples Tipos de Documentos**: Soporte para PDF, Markdown y HTML

---

## ğŸ—ï¸ Arquitectura del Sistema

```
docs-to-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extractor.py          # Coordinador de extracciÃ³n de documentos
â”‚   â”‚   â”œâ”€â”€ chunker.py            # SegmentaciÃ³n inteligente de texto
â”‚   â”‚   â”œâ”€â”€ summarizer.py         # Resumen jerÃ¡rquico de documentos
â”‚   â”‚   â””â”€â”€ parsers/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ markdown_parser.py # Procesador de Markdown
â”‚   â”‚       â”œâ”€â”€ pdf_parser.py      # Wrapper de Docling para PDF
â”‚   â”‚       â””â”€â”€ html_parser.py     # Procesador de contenido HTML
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py         # GeneraciÃ³n de embeddings con Ollama
â”‚   â”‚   â””â”€â”€ store.py              # Base de datos vectorial local FAISS
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ retriever.py          # LÃ³gica de recuperaciÃ³n de documentos
â”‚   â”‚   â””â”€â”€ generator.py          # GeneraciÃ³n de respuestas con Ollama
â”‚   â””â”€â”€ cli/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ commands.py           # Definiciones de comandos CLI
â”‚       â””â”€â”€ chat.py               # Interfaz de chat interactivo
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â””â”€â”€ test_rag.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/                # Documentos fuente
â”‚   â””â”€â”€ vector_db/                # Base de datos vectorial FAISS
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py               # ConfiguraciÃ³n del sistema
â”œâ”€â”€ pyproject.toml               # Dependencias del proyecto y configuraciÃ³n CLI
â”œâ”€â”€ pyproject.toml               # Dependencias y configuraciÃ³n del proyecto
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

---

## ğŸ”§ Stack TecnolÃ³gico

### Dependencias Principales
- **Python 3.10+** - Lenguaje de programaciÃ³n principal
- **LangChain** - Framework de LLM con integraciÃ³n Ollama
- **Docling** - ExtracciÃ³n robusta de PDF y documentos
- **FAISS** - BÃºsqueda local por similitud vectorial
- **Ollama Python** - Cliente LLM local
- **Click** - Framework CLI profesional
- **Rich** - Salida de terminal hermosa

### Procesamiento de Documentos
- **BeautifulSoup4** - AnÃ¡lisis y limpieza de HTML
- **markdown** - Procesamiento avanzado de Markdown

### Herramientas de Desarrollo
- **uv** - GestiÃ³n rÃ¡pida de dependencias
- **ruff** - Linting y formateo de cÃ³digo
- **pytest** - Framework de testing
- **mypy** - VerificaciÃ³n de tipos

### Modelos Ollama Recomendados
- **Embeddings (InglÃ©s)**: `nomic-embed-text:v1.5` (274MB) - Embeddings rÃ¡pidos y eficientes
- **Embeddings (EspaÃ±ol)**: `jina/jina-embeddings-v2-base-es:latest` (560MB) - Optimizado para espaÃ±ol
- **Embeddings (MultilingÃ¼e)**: `mxbai-embed-large:latest` (670MB) - Soporte multilingÃ¼e general
- **Chat/GeneraciÃ³n**: `llama3.2:latest` (2GB) - Equilibrio velocidad/calidad para RAG y resumen

---

## ğŸ›ï¸ ConfiguraciÃ³n

### ConfiguraciÃ³n del Sistema (`config/settings.py`)

```python
@dataclass
class Settings:
    """ConfiguraciÃ³n principal del sistema."""
    
    # === MODELOS OLLAMA ===
    embedding_model: str = "nomic-embed-text:v1.5"  # Embeddings rÃ¡pidos y eficientes
    embedding_model_multilingual: str = "jina/jina-embeddings-v2-base-es:latest"  # EspecÃ­fico para espaÃ±ol
    chat_model: str = "llama3.2:latest"             # Modelo de chat/generaciÃ³n
    ollama_base_url: str = "http://localhost:11434"
    
    # === CONFIGURACIÃ“N DE IDIOMA ===
    auto_detect_language: bool = True            # Cambio automÃ¡tico para contenido no inglÃ©s
    primary_language: str = "es"                 # Idioma principal (es, en, fr, etc.)
    
    # === PROCESAMIENTO DE DOCUMENTOS ===
    chunk_size: int = 1000                       # TamaÃ±o de chunk de texto
    chunk_overlap: int = 200                     # Solapamiento de chunks
    supported_extensions: List[str] = [".pdf", ".md", ".html"]
    
    # === PARÃMETROS RAG ===
    top_k_retrieval: int = 3                     # Documentos a recuperar (auto-ajustado)
    similarity_threshold: float = 0.7            # Umbral de similitud (auto-ajustado)
    max_tokens_response: int = 512               # Tokens mÃ¡ximos de respuesta
    temperature: float = 0.1                     # Temperatura del LLM
    
    # === PARÃMETROS RAG ADAPTATIVOS ===
    enable_adaptive_params: bool = True          # Auto-ajustar basado en tamaÃ±o de BD
    min_similarity_threshold: float = 0.3       # Umbral mÃ­nimo para BDs grandes
    max_top_k: int = 20                         # MÃ¡ximo de chunks para BDs grandes
    
    # === CONFIGURACIÃ“N DE RESUMEN ===
    enable_summarization: bool = True           # Generar resÃºmenes jerÃ¡rquicos
    summarization_model: str = "llama3.2:latest"  # Modelo dedicado para resÃºmenes
    summarization_temperature: float = 0.1     # Temperatura mÃ¡s baja para resÃºmenes fieles
    enable_summary_validation: bool = True     # ValidaciÃ³n de calidad para resÃºmenes
    
    # === RUTAS ===
    documents_path: str = "data/documents"
    vector_db_path: str = "data/vector_db"
```

---

## ğŸ–¥ï¸ Interfaz CLI

### Comandos Principales

```bash
# ConfiguraciÃ³n inicial
docs-to-rag setup

# GestiÃ³n de documentos (funcionalidad CORE)
docs-to-rag add ./ruta/a/documentos/          # Procesar y aÃ±adir documentos
docs-to-rag list                              # Listar documentos indexados
docs-to-rag clear                             # Limpiar base de datos vectorial
docs-to-rag stats                             # EstadÃ­sticas del sistema
docs-to-rag enrich ./documento.pdf           # Generar resÃºmenes jerÃ¡rquicos

# Chat RAG (funcionalidad CORE)
docs-to-rag chat                              # Chat interactivo
docs-to-rag query "Â¿QuÃ© dice sobre X?"       # Consulta directa

# Operaciones avanzadas
docs-to-rag reprocess ./doc.pdf               # Reprocesar documento con chunking mejorado
docs-to-rag reprocess ./doc.pdf --multilingual # Usar modelo de embedding multilingÃ¼e
docs-to-rag inspect                           # Inspeccionar chunks almacenados para debug
docs-to-rag config                            # Mostrar/modificar configuraciÃ³n del sistema

# Utilidades
docs-to-rag --help                           
docs-to-rag --version                        
```

### Detalles de Comandos

#### `setup`
- Verifica conexiÃ³n con Ollama
- Crea estructura de directorios
- Verifica disponibilidad de modelos requeridos
- Proporciona guÃ­a de configuraciÃ³n si encuentra problemas

#### `add <ruta>`
- Procesa documentos recursivamente desde la ruta
- Soporta archivos PDF, Markdown y HTML
- Extrae texto y crea chunks inteligentes
- Genera embeddings y los almacena en FAISS
- Muestra progreso de procesamiento y estadÃ­sticas

#### `list`
- Lista todos los documentos indexados en la base de conocimiento
- Muestra rutas de documentos y metadatos bÃ¡sicos
- Muestra conteos totales de documentos y chunks
- Ayuda a verificar quÃ© contenido estÃ¡ disponible para consultas
- Vista rÃ¡pida de contenidos del sistema

#### `clear`
- Elimina todos los documentos indexados de la base de datos vectorial
- Requiere confirmaciÃ³n del usuario para prevenir pÃ©rdida accidental de datos
- Limpia tanto chunks de documentos como embeddings
- Reinicia el sistema a estado vacÃ­o
- Ãštil para empezar de nuevo o actualizaciones mayores de contenido

#### `chat`
- Inicia sesiÃ³n de chat interactivo con interfaz de terminal rica
- Muestra proceso de recuperaciÃ³n (documentos encontrados)
- Muestra respuestas generadas con formato markdown
- Muestra documentos fuente para transparencia
- Soporta comandos internos: `/help`, `/stats`, `/sources`, `/history`, `/clear`, `/exit`
- Mantiene historial de conversaciÃ³n durante la sesiÃ³n
- Proporciona verificaciones de estado y preparaciÃ³n del sistema
- Permite alternar visualizaciÃ³n de fuentes e informaciÃ³n de debug

#### `query <pregunta>`
- Respuesta a pregunta de una sola vez
- Ãštil para scripting y automatizaciÃ³n
- Retorna respuesta con informaciÃ³n bÃ¡sica de fuentes

#### `stats`
- Conteo de documentos y estadÃ­sticas de chunks
- TamaÃ±o de base de datos vectorial
- InformaciÃ³n de modelos
- MÃ©tricas de procesamiento

#### `enrich <ruta_documento>`
- Genera resÃºmenes jerÃ¡rquicos para recuperaciÃ³n mejorada
- Crea resÃºmenes a nivel de documento, capÃ­tulo y concepto
- Requiere que el resumen estÃ© habilitado (`docs-to-rag config --enable-summaries`)
- Usa modelo de resumen especializado con parÃ¡metros optimizados
- Almacena chunks de resumen sintÃ©tico en base de datos vectorial
- Mejora significativamente los resultados para preguntas conceptuales y amplias
- ValidaciÃ³n de calidad opcional con verificaciÃ³n de fidelidad
- Muestra progreso de procesamiento y estadÃ­sticas de resumen

#### `reprocess <ruta_documento>`
- Limpia base de datos vectorial y reprocesa un documento especÃ­fico
- Ãštil para aplicar algoritmos de chunking o configuraciones actualizadas
- Soporta flag `--multilingual` para cambiar modelos de embedding
- Reconstruye embeddings e Ã­ndices desde cero
- Muestra progreso de procesamiento y estadÃ­sticas finales
- Mantiene metadatos de documentos y asociaciones de archivos

#### `inspect`
- Muestra chunks reales almacenados en la base de datos vectorial
- Ayuda a debuggear calidad de contenido y comportamiento de chunking
- Soporta parÃ¡metro `--count` para limitar nÃºmero de chunks mostrados
- Soporta parÃ¡metro `--search` para filtrar chunks por contenido
- Muestra metadatos de chunks incluyendo archivo fuente, posiciÃ³n y tipo
- Herramienta esencial para entender comportamiento del sistema y resoluciÃ³n de problemas

#### `config`
- Muestra configuraciÃ³n actual del sistema cuando se llama sin parÃ¡metros
- Permite modificaciÃ³n de configuraciones vÃ­a flags de lÃ­nea de comandos
- Soporta configuraciÃ³n de idioma (`--language es`)
- Controla caracterÃ­sticas de resumen (`--enable-summaries`, `--disable-summaries`)
- Gestiona validaciÃ³n de calidad (`--enable-validation`, `--disable-validation`)
- Establece modelos especializados (`--summary-model llama3.2:latest`)
- Guarda cambios en `config/user_config.json` local
- Muestra configuraciÃ³n detallada incluyendo modelos, embeddings y parÃ¡metros de procesamiento

---

## ğŸ” Pipeline de Procesamiento de Documentos

### Formatos Soportados

#### 1. Markdown (`.md`)
- Preserva estructura de encabezados para contexto
- Extrae metadatos de frontmatter
- Maneja bloques de cÃ³digo y enlaces apropiadamente
- Chunking inteligente respetando secciones

#### 2. PDF (`.pdf`)
- Usa Docling para extracciÃ³n robusta
- Maneja layouts complejos y tablas
- Preserva estructura de documento
- Extrae metadatos cuando estÃ¡n disponibles

#### 3. HTML (`.html`)
- Limpia contenido (elimina scripts, estilos)
- Preserva estructura semÃ¡ntica
- Extrae metadatos de pÃ¡gina (tÃ­tulo, descripciÃ³n)
- Maneja varias codificaciones HTML

### Estrategia de Chunking de Texto

```python
def chunk_document(content: str, doc_type: str) -> List[Chunk]:
    """
    Chunking inteligente basado en tipo de documento:
    
    - Markdown: Respeta lÃ­mites de secciÃ³n (encabezados)
    - PDF: Chunking consciente de pÃ¡rrafos
    - HTML: LÃ­mites de elementos semÃ¡nticos
    - Todos: TamaÃ±o configurable con solapamiento
    """
```

---

## ğŸ¤– ImplementaciÃ³n RAG

### Pipeline Principal (`rag/pipeline.py`)

```python
class RAGPipeline:
    """Pipeline principal de procesamiento RAG."""
    
    def ask(self, question: str) -> RAGResult:
        """
        Flujo completo de RAG:
        1. Generar embedding de la pregunta
        2. Buscar en base de datos vectorial chunks similares
        3. Recuperar documentos relevantes top-k
        4. Generar respuesta usando contexto
        5. Retornar resultado con metadatos
        """
```

### Proceso de RecuperaciÃ³n
1. **Embedding de Consulta**: Convertir pregunta a vector usando Ollama
2. **BÃºsqueda por Similitud**: BÃºsqueda de similitud coseno FAISS
3. **SelecciÃ³n de Contexto**: Chunks mÃ¡s relevantes top-k
4. **Seguimiento de Fuentes**: Mantener informaciÃ³n de fuente de documento

### Proceso de GeneraciÃ³n
1. **ConstrucciÃ³n de Prompt**: Construir prompt RAG con contexto
2. **GeneraciÃ³n Local**: Usar Ollama para generaciÃ³n de respuesta
3. **Procesamiento de Respuesta**: Limpiar y formatear salida
4. **RecolecciÃ³n de Metadatos**: Seguir timing y fuentes

---

## ğŸ“Š Componentes Educativos

### Objetivos de Aprendizaje

1. **ExtracciÃ³n de Documentos**
   - Entender desafÃ­os de diferentes formatos de archivo
   - Aprender tÃ©cnicas de extracciÃ³n robustas
   - Manejar varias estructuras de documentos

2. **Chunking de Texto**
   - Importancia del tamaÃ±o de chunk y solapamiento
   - Estrategias de segmentaciÃ³n conscientes del contenido
   - Impacto en calidad de recuperaciÃ³n

3. **Embeddings Vectoriales**
   - Convertir texto a representaciones numÃ©ricas
   - Entender similitud semÃ¡ntica
   - Modelos de embedding locales vs. en la nube

4. **BÃºsqueda Vectorial**
   - Indexado y bÃºsqueda FAISS
   - MÃ©tricas de similitud (similitud coseno)
   - Consideraciones de rendimiento

5. **ImplementaciÃ³n RAG**
   - Conceptos de GeneraciÃ³n Aumentada por RecuperaciÃ³n
   - GestiÃ³n de ventana de contexto
   - IngenierÃ­a de prompts para RAG

### Estructura de CÃ³digo para Aprendizaje

- **Comentarios Exhaustivos**: Cada funciÃ³n explica su propÃ³sito y decisiones
- **Anotaciones de Tipo**: Todas las funciones tienen hints de tipo completos
- **DiseÃ±o Modular**: SeparaciÃ³n clara de responsabilidades
- **Logging Educativo**: Salida verbosa mostrando cada paso de procesamiento
- **MÃ©tricas Simples**: Tiempos de procesamiento, conteos de chunks, puntajes de similitud

---

## ğŸš€ Ejemplos de Uso

### Flujo Completo

```bash
# 1. ConfiguraciÃ³n inicial (solo una vez)
docs-to-rag setup
# ğŸ” Verificando Ollama...
# ğŸ“ Creando directorios...
# ğŸ¤– Verificando modelos...
# âœ… ConfiguraciÃ³n completada!

# 2. AÃ±adir documentos al sistema
docs-to-rag add ./mi_documentacion/
# ğŸ“„ Procesando documentos...
# âœ… 5 documentos procesados
# ğŸ“Š 142 chunks creados
# ğŸ’¾ Base de datos vectorial actualizada

# 3. Verificar quÃ© fue procesado
docs-to-rag stats
# ğŸ“š Documentos indexados: 5
# ğŸ§© Total de chunks: 142
# ğŸ’¾ TamaÃ±o BD vectorial: 3.2MB

# 4. Chat interactivo
docs-to-rag chat
# ğŸ¤– Chat RAG iniciado!
# ğŸ“ Tu pregunta: Â¿CÃ³mo instalo esto?
# ğŸ” Buscando documentos relevantes...
# ğŸ“„ Encontrados 3 documentos relevantes
# ğŸ’­ Generando respuesta...
# ğŸ¤– Respuesta: Para instalar el software...

# 5. Consulta directa
docs-to-rag query "Resume las caracterÃ­sticas principales"
# ğŸ” Procesando consulta...
# ğŸ¤– Respuesta: Las caracterÃ­sticas principales incluyen...
```

---

## ğŸ§ª Desarrollo y Testing

### Dependencias del Proyecto (`pyproject.toml`)

```toml
[project]
name = "docs-to-rag"
version = "0.1.0"
description = "Sistema RAG educativo con LLMs locales"
authors = [
    {name = "Juanje Ojeda", email = "juanje@redhat.com"}
]
requires-python = ">=3.10"

dependencies = [
    "langchain>=0.1.0",
    "langchain-ollama>=0.1.0",
    "docling>=1.0.0",
    "faiss-cpu>=1.7.4",
    "click>=8.1.0",
    "rich>=13.0.0",
    "beautifulsoup4>=4.12.0",
    "markdown>=3.5.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.5.0",
    "httpx>=0.25.0",
    "aiofiles>=23.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "ruff>=0.1.0",
    "mypy>=1.7.0",
    "pre-commit>=3.0.0",
]

[project.scripts]
docs-to-rag = "src.cli.commands:main"
```

### Estrategia de Testing

- **Tests Unitarios**: Cada componente (extractor, embeddings, recuperaciÃ³n)
- **Tests de IntegraciÃ³n**: Funcionalidad completa del pipeline
- **Tests de Documentos**: Manejo de varios formatos de archivo
- **Tests de Rendimiento**: Velocidad de procesamiento y uso de memoria

---

## ğŸ¯ Criterios de Ã‰xito

### Requisitos Funcionales

1. **Procesamiento de Documentos**: Extraer y fragmentar exitosamente archivos PDF, MD, HTML
2. **Almacenamiento Vectorial**: Crear y buscar en base de datos vectorial FAISS
3. **Pipeline RAG**: Recuperar documentos relevantes y generar respuestas coherentes
4. **Interfaz CLI**: InteracciÃ³n intuitiva por lÃ­nea de comandos
5. **OperaciÃ³n Local**: Sin dependencias de APIs externas

### Requisitos Educativos

1. **Claridad de CÃ³digo**: ImplementaciÃ³n bien documentada y legible
2. **Ruta de Aprendizaje**: ProgresiÃ³n clara de conceptos simples a complejos
3. **ExperimentaciÃ³n**: FÃ¡cil modificar parÃ¡metros y ver resultados
4. **Entendimiento**: Cada paso explica por quÃ©, no solo cÃ³mo

### Objetivos de Rendimiento

- **Procesamiento de Documentos**: < 30 segundos para 10 documentos tÃ­picos
- **Respuesta a Consultas**: < 5 segundos para consultas RAG tÃ­picas
- **Uso de Memoria**: < 1GB RAM para colecciones moderadas de documentos
- **Almacenamiento**: TamaÃ±o eficiente de base de datos vectorial

---

## ğŸ”„ Fases de Desarrollo

### Fase 1: Base Principal
- ExtracciÃ³n bÃ¡sica de documentos (archivos de texto primero)
- Estrategia simple de chunking
- IntegraciÃ³n Ollama para embeddings
- Almacenamiento y recuperaciÃ³n bÃ¡sica FAISS

### Fase 2: Soporte de Documentos
- ExtracciÃ³n PDF con Docling
- Procesamiento Markdown con preservaciÃ³n de estructura
- ExtracciÃ³n de contenido HTML
- Estrategias mejoradas de chunking

### Fase 3: ImplementaciÃ³n RAG
- Pipeline completo de recuperaciÃ³n
- GeneraciÃ³n de respuestas con contexto
- Desarrollo de interfaz CLI
- Funcionalidad de chat interactivo

### Fase 4: Pulido y EducaciÃ³n
- DocumentaciÃ³n exhaustiva
- Documentos de ejemplo y casos de uso
- OptimizaciÃ³n de rendimiento
- Testing y validaciÃ³n

---

## ğŸ“š Consideraciones Adicionales

### Prerrequisitos

- **Ollama**: Debe estar instalado y ejecutÃ¡ndose (`ollama serve`)
- **Modelos**: Modelos requeridos descargados (`nomic-embed-text:v1.5`, `llama3.2:latest`)
- **Python**: VersiÃ³n 3.10 o superior
- **Almacenamiento**: Al menos 500MB de espacio libre para modelos y datos

### Limitaciones

- **Modelos Locales**: El rendimiento depende del hardware local
- **Tipos de Documentos**: Limitado a PDF, Markdown, HTML inicialmente
- **Escala**: DiseÃ±ado para uso educativo, no escala de producciÃ³n
- **Idiomas**: Optimizado para contenido en espaÃ±ol/inglÃ©s

### Extensiones

Mejoras futuras potenciales:
- Formatos de documentos adicionales (DOCX, TXT)
- ComparaciÃ³n de mÃºltiples modelos de embedding
- Estrategias avanzadas de chunking
- OpciÃ³n de interfaz web
- MÃ©tricas de evaluaciÃ³n y benchmarks

---

## ğŸ”§ CaracterÃ­sticas de OptimizaciÃ³n RAG (v0.2)

### Sistema de ParÃ¡metros Adaptativos
El sistema ajusta automÃ¡ticamente los parÃ¡metros de recuperaciÃ³n basÃ¡ndose en el tamaÃ±o de la base de datos vectorial:

- **Bases de datos pequeÃ±as (â‰¤100 chunks)**: `top_k=3`, `threshold=0.7` (restrictivo)
- **Bases de datos medianas (â‰¤1K chunks)**: `top_k=5`, `threshold=0.6` (equilibrado)
- **Bases de datos grandes (â‰¤5K chunks)**: `top_k=10`, `threshold=0.5` (relajado)
- **Bases de datos muy grandes (>5K chunks)**: `top_k=15-20`, `threshold=0.3-0.4` (permisivo)

### Herramientas de Debug y DiagnÃ³stico
- **Modo Debug**: Flag `--debug` muestra parÃ¡metros adaptativos y detalles de recuperaciÃ³n
- **ParÃ¡metros Personalizados**: Sobrescribir configuraciones automÃ¡ticas con `--top-k` y `--threshold`
- **MÃ©tricas de Rendimiento**: InformaciÃ³n detallada de timing y uso de chunks
- **Puntajes de Similitud**: VisualizaciÃ³n opcional de puntajes de relevancia de documentos

### Mejoras CLI
```bash
# OptimizaciÃ³n automÃ¡tica
docs-to-rag query "Pregunta" --debug

# Ajuste manual de parÃ¡metros
docs-to-rag query "Pregunta" --top-k 20 --threshold 0.2

# ResoluciÃ³n de problemas con resultados pobres
docs-to-rag query "Pregunta" --threshold 0.1  # MÃ¡s permisivo
```

Esto asegura rendimiento Ã³ptimo a travÃ©s de colecciones de documentos de cualquier tamaÃ±o, desde pequeÃ±as demos hasta grandes bases de conocimiento empresariales.

---

## ğŸŒ DetecciÃ³n AutomÃ¡tica de Idioma (v0.3)

### Resumen
El sistema ahora detecta automÃ¡ticamente contenido en espaÃ±ol y optimiza los modelos de embedding consecuentemente, mejorando significativamente la calidad de recuperaciÃ³n para documentos en espaÃ±ol.

### CaracterÃ­sticas de DetecciÃ³n de Idioma
- **DetecciÃ³n a Nivel de Documento**: Analiza contenido durante extracciÃ³n usando frecuencia de palabras clave en espaÃ±ol
- **DetecciÃ³n a Nivel de Consulta**: Identifica consultas en espaÃ±ol en tiempo de ejecuciÃ³n
- **Cambio AutomÃ¡tico de Modelo**: Cambia sin problemas entre modelos de embedding basÃ¡ndose en idioma
- **OptimizaciÃ³n por Lotes**: Usa embeddings en espaÃ±ol cuando la mayorÃ­a de documentos estÃ¡n en espaÃ±ol (â‰¥50%)

### Modelos de Embedding
- **InglÃ©s/Por Defecto**: `nomic-embed-text:v1.5` (rÃ¡pido, eficiente)
- **Especializado en EspaÃ±ol**: `jina/jina-embeddings-v2-base-es:latest` (optimizado para espaÃ±ol)
- **Fallback MultilingÃ¼e**: `mxbai-embed-large:latest` (soporte multilingÃ¼e general)

### Algoritmo de DetecciÃ³n
La detecciÃ³n de espaÃ±ol se basa en analizar la frecuencia de palabras comunes en espaÃ±ol:
- **ArtÃ­culos**: el, la, los, las
- **Preposiciones**: de, del, en, con, por, para
- **Pronombres**: que, se, una, un
- **Verbos**: es, son, estÃ¡, estÃ¡n
- **Adverbios**: tambiÃ©n, muy, mÃ¡s
- **Conectores**: como, cuando, donde, porque

El contenido se considera espaÃ±ol si >15% de las palabras coinciden con palabras clave en espaÃ±ol.

### Ejemplos de Uso
```bash
# DetecciÃ³n automÃ¡tica durante procesamiento de documentos
docs-to-rag add "documento_spanish.pdf"  # Auto-detecta espaÃ±ol â†’ usa modelo jina

# Procesamiento multilingÃ¼e explÃ­cito
docs-to-rag reprocess "documento.pdf" --multilingual

# Consulta con detecciÃ³n automÃ¡tica de idioma
docs-to-rag query "Â¿CuÃ¡l es el contenido principal?"  # Auto-detecta consulta en espaÃ±ol

# ConfiguraciÃ³n de idioma
docs-to-rag config --language es  # Establecer idioma principal a espaÃ±ol
docs-to-rag config                # Mostrar configuraciones actuales de idioma
```

### Impacto en Rendimiento
- **Documentos en EspaÃ±ol**: 40-60% mejora en precisiÃ³n de recuperaciÃ³n
- **Colecciones de Idioma Mixto**: SelecciÃ³n inteligente de modelo por documento/consulta
- **Consultas Inter-idioma**: DegradaciÃ³n elegante con modelos multilingÃ¼es
- **Tiempo de Procesamiento**: Overhead mÃ­nimo (~2-3% aumento para detecciÃ³n de idioma)

---

## ğŸ§  Enriquecimiento JerÃ¡rquico de Documentos

### Resumen

El comando `enrich` implementa un sofisticado sistema de resumen de documentos que genera mÃºltiples tipos de resÃºmenes sintÃ©ticos para mejorar dramÃ¡ticamente la calidad de recuperaciÃ³n para preguntas conceptuales. Esta caracterÃ­stica transforma el sistema RAG de un simple recuperador basado en chunks a una base de conocimiento multi-nivel.

### Tipos de Resumen JerÃ¡rquico

#### 1. ResÃºmenes a Nivel de Documento
- **PropÃ³sito**: Entendimiento general del documento y consultas de temas amplios
- **Contenido**: Resumen ejecutivo, temas principales, conclusiones clave
- **Caso de Uso**: "Â¿De quÃ© trata este documento?", "Resume los puntos principales"

#### 2. ResÃºmenes a Nivel de CapÃ­tulo
- **PropÃ³sito**: RecuperaciÃ³n de informaciÃ³n especÃ­fica de secciÃ³n
- **Contenido**: ResÃºmenes de capÃ­tulo/secciÃ³n con contexto estructural
- **Caso de Uso**: "Â¿QuÃ© discute el CapÃ­tulo 3?", "Explica la secciÃ³n de metodologÃ­a"

#### 3. ResÃºmenes a Nivel de Concepto
- **PropÃ³sito**: RecuperaciÃ³n enfocada de temas y definiciones
- **Contenido**: Conceptos clave, definiciones y conocimiento especializado
- **Caso de Uso**: "Define machine learning", "Explica los conceptos clave"

### ConfiguraciÃ³n Especializada de LLM para Resumen Fiel

El sistema usa configuraciones dedicadas de LLM optimizadas especÃ­ficamente para generaciÃ³n de resÃºmenes, asegurando mayor calidad y resÃºmenes mÃ¡s fieles comparado con usar parÃ¡metros generales de chat.

#### ParÃ¡metros EspecÃ­ficos para Resumen
- **Modelo Dedicado**: Puede usar modelo diferente que respuestas de chat (`summarization_model`)
- **Temperatura MÃ¡s Baja**: `0.1` para resÃºmenes mÃ¡s determinÃ­sticos y fieles
- **Muestreo Enfocado**: `top_p=0.8` para selecciÃ³n de contenido mÃ¡s enfocada
- **LÃ­mites de Tokens Optimizados**:
  - ResÃºmenes de documento: 400 tokens
  - ResÃºmenes de capÃ­tulo: 250 tokens
  - ResÃºmenes de concepto: 200 tokens

#### Sistema de Control de Calidad
- **Prompting Mejorado**: Prompts del sistema diseÃ±ados especÃ­ficamente para resumen fiel
- **GeneraciÃ³n Multi-intento**: Hasta 3 intentos por resumen con puntuaciÃ³n de calidad
- **ValidaciÃ³n de Fidelidad**: VerificaciÃ³n automÃ¡tica de fidelidad del contenido al material fuente
- **MÃ©tricas de Calidad**:
  - ValidaciÃ³n de longitud
  - VerificaciÃ³n de estructura
  - Consistencia de idioma
  - AnÃ¡lisis de solapamiento de contenido
  - DetecciÃ³n de meta-comentarios

#### CaracterÃ­sticas de Fidelidad de Resumen
- **AplicaciÃ³n de Requisitos CrÃ­ticos**: Instrucciones explÃ­citas contra alucinaciÃ³n
- **VerificaciÃ³n de Fidelidad de Fuente**: Valida contenido de resumen contra texto original
- **PuntuaciÃ³n de Calidad**: Sistema de puntuaciÃ³n 0.0-1.0 con umbral 0.8 para aceptaciÃ³n
- **Reintento AutomÃ¡tico**: ResÃºmenes de pobre calidad regenerados automÃ¡ticamente
- **AnÃ¡lisis de Solapamiento**: Asegura 30-70% solapamiento de palabras clave con fuente (previene tanto copia como alucinaciÃ³n)

### Opciones de ConfiguraciÃ³n
```bash
# Habilitar validaciÃ³n de calidad (mÃ¡s lento pero mÃ¡s confiable)
docs-to-rag config --enable-validation

# Usar modelo especializado para resÃºmenes
docs-to-rag config --summary-model "llama3.1:8b"

# Mostrar configuraciÃ³n detallada de resumen
docs-to-rag config
```

### Flujo de Enriquecimiento

#### Pipeline de Procesamiento
1. **AnÃ¡lisis de Documento**: Extraer y analizar estructura de documento
2. **DetecciÃ³n de CapÃ­tulos**: Identificar secciones, encabezados y divisiones lÃ³gicas
3. **ExtracciÃ³n de Conceptos**: Identificar tÃ©rminos y conceptos clave usando PLN
4. **GeneraciÃ³n de Resumen**: Crear resÃºmenes jerÃ¡rquicos usando prompts especializados
5. **ValidaciÃ³n de Calidad**: VerificaciÃ³n opcional de fidelidad y calidad
6. **Almacenamiento Vectorial**: Convertir resÃºmenes a chunks y almacenar con embeddings

#### Proceso de Enriquecimiento de Ejemplo
```bash
# 1. Habilitar resumen
docs-to-rag config --enable-summaries

# 2. Enriquecer un documento
docs-to-rag enrich ./paper_investigacion.pdf

# Salida:
# âœ… Enriquecimiento completado exitosamente
# Documentos procesados: 1
# Chunks de resumen generados: 8
#   - ResÃºmenes de documento: 1
#   - ResÃºmenes de capÃ­tulo: 4
#   - ResÃºmenes de concepto: 3
```

#### Impacto en RecuperaciÃ³n
- **Antes del Enriquecimiento**: Solo chunks de texto original disponibles
- **DespuÃ©s del Enriquecimiento**: Chunks originales + chunks de resumen sintÃ©tico
- **Mejora de Consultas**: 30-50% mejores resultados para preguntas conceptuales
- **Casos de Uso Mejorados**:
  - Preguntas de revisiÃ³n de literatura
  - Explicaciones de conceptos
  - ResÃºmenes de documentos
  - ResÃºmenes de secciones

### Compromisos Calidad vs Velocidad
- **Modo Por Defecto**: GeneraciÃ³n rÃ¡pida, validaciÃ³n bÃ¡sica
- **Modo de Calidad**: GeneraciÃ³n multi-intento con validaciÃ³n (2-3x mÃ¡s lento, mucho mayor calidad)
- **Modelo Especializado**: Puede usar modelo mÃ¡s grande/mejor solo para resÃºmenes mientras mantiene modelo rÃ¡pido para chat

Esta configuraciÃ³n avanzada aborda el desafÃ­o crÃ­tico de fidelidad de resumen mientras mantiene los beneficios de rendimiento del despliegue local de LLM.

---

*Autor: Juanje Ojeda (juanje@redhat.com)*  
*Proyecto: Sistema RAG Educativo con LLMs Locales*
